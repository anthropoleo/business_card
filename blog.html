<!DOCTYPE html>

<!-- 
This is the main layout template for all pages on my site.
It sets up the basic HTML structure and loads all the CSS/JS.
I've added a dropdown menu in the top right for navigation.
-->

<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Leo's business card</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Blog" />
<meta name="author" content="Leo" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Thoughts and insights on AI, data science, and technology from Leo - Gold Coast-based data scientist." />
<meta property="og:description" content="Thoughts and insights on AI, data science, and technology from Leo - Gold Coast-based data scientist." />
<link rel="canonical" href="https://anthropoleo.github.io/hacked-jekyll/blog.html" />
<meta property="og:url" content="https://anthropoleo.github.io/hacked-jekyll/blog.html" />
<meta property="og:site_name" content="Leo’s business card" />
<meta property="og:image" content="https://anthropoleo.github.io/hacked-jekyll/assets/images/profile.jpg" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://anthropoleo.github.io/hacked-jekyll/assets/images/profile.jpg" />
<meta property="twitter:title" content="Blog" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Leo"},"description":"Thoughts and insights on AI, data science, and technology from Leo - Gold Coast-based data scientist.","headline":"Blog","image":"https://anthropoleo.github.io/hacked-jekyll/assets/images/profile.jpg","url":"https://anthropoleo.github.io/hacked-jekyll/blog.html"}</script>
<!-- End Jekyll SEO tag -->
 <!-- Using the SEO plugin but keeping my custom title -->
  <link rel="icon" type="image/x-icon" href="/hacked-jekyll/assets/favicon.ico" />
  <link rel="apple-touch-icon" sizes="180x180" href="/hacked-jekyll/assets/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/hacked-jekyll/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/hacked-jekyll/assets/favicon-16x16.png">
  <link rel="manifest" href="/hacked-jekyll/assets/site.webmanifest">
  <link rel="stylesheet" href="/hacked-jekyll/assets/css/normalize.css" />
  <link rel="stylesheet" href="/hacked-jekyll/assets/css/open-color.css" />
  <link rel="stylesheet" href="/hacked-jekyll/assets/css/styles.css" />
  <link rel="stylesheet" href="/hacked-jekyll/assets/css/mobile-optimizations.css" />
  <link rel="stylesheet" href="/hacked-jekyll/assets/css/navigation.css" />
  <script src="/hacked-jekyll/assets/js/typed.umd.js"></script>
  <script src="/hacked-jekyll/assets/js/terminal-game.js"></script>
</head>

<body>
  <!-- Navigation dropdown -->
  <div class="nav-container">
    <div class="dropdown">
      <button class="dropbtn">Menu</button>
      <div class="dropdown-content">
        <a href="/hacked-jekyll/">Home</a>
        <a href="/hacked-jekyll/about">About</a>
        <a href="/hacked-jekyll/blog">Blog</a>
        <a href="/hacked-jekyll/projects">Projects</a>
        <a href="/hacked-jekyll/contact">Contact</a>
      </div>
    </div>
  </div>

  <main>
<!-- 
This is the layout that handles the JSON-style formatting for all my pages.
It figures out which data file to use based on the page name (index, about, blog, etc).
I've set it up so each page can have its own data but keep the same JSON look.

If I need to add a new page, I just need to:
1. Create a new .md file (like about.md)
2. Create a matching data file in _data folder (like about.yml)
3. The page will automatically use that data file
-->




<div id="json">
  
  <div class="my ms">
    "<span class="key">Blog</span>":
    "<span class="value">My thoughts on data science and AI</span>",
  </div>
  
  <div class="my ms">
    "<span class="key">Latest Posts</span>":
    "<span class="value">LLM agents: promise, pitfalls, and a pinch of realism
Large-language-model (LLM) agents are everywhere right now. Scroll LinkedIn for five minutes and you’ll see “autonomous AI assistants” that supposedly write code, book flights, and build marketing plans while you relax with a flat white. I’ve spent the past year stress-testing these claims. The upshot? Agents are genuinely useful—but only when you understand their limits and design around them. Below is a sober tour of why they stumble, why they’re still worth the trouble, and how to keep both feet on the ground while you build with them.

Why LLMs act weird in the first place
Before we talk agents, it’s worth unpacking what an LLM actually does.

Next-token prediction, not deep reasoning
At training time a transformer is shown trillions of sentences and learns to guess the next token (roughly a word-piece) given the previous ones. That’s it. All the reasoning, knowledge retrieval, translation and coding we marvel at emerge from this statistical game. When the distribution shifts—unusual wording, niche domains, half-finished sentences—the model may still “fill in” something that looks plausible but is in fact nonsense. Hallucinations aren’t a bug: they’re a by-product of predicting “the most likely continuation”.

Transformers were invented for translation
Vaswani et al.’s Attention Is All You Need (2017) framed the architecture as a machine-translation engine. It just happened that the self-attention mechanism scaled beautifully, so researchers applied it to every text task under the sun. The takeaway: LLMs were never designed for stable, multi-step planning. When we bolt an “agent loop” on top—think, act, observe, repeat—we’re asking a translator-turned-autocomplete system to behave like a decision-making robot. Sometimes it works spectacularly; other times you get an in-depth answer about a physics paper that doesn’t exist.

Short memories and context cliffs
Even GPT-4o’s beefy context window tops out at 128 k tokens. That sounds gigantic until you need an agent to read a pile of PDFs, remember user preferences, and maintain a running rationale. If you push too much text in, the model forgets older bits (they’re pushed out of the window) or drowns in irrelevant details. If you push too little, it loses crucial facts. Either way, continuity breaks.

Compounding error
Autoregressive generation means every token depends on the ones before. A tiny error early in an agent’s chain of thought can snowball. Picture an agent that decides, after step three, that the project budget is £50 k instead of £5 k. Every downstream calculation is now off by an order of magnitude.

Lack of grounded feedback
Traditional software returns explicit error codes; an LLM returns another paragraph of prose. If the agent misunderstands a tool response—say an API throws “401 unauthorised”—it may merrily rephrase that as “looks good, we’re authorised” unless you catch it with careful parsing and guard-rails.

Why out-of-the-box frameworks feel both brilliant and brittle
Frameworks such as LangChain, AutoGPT and CrewAI give you:

✦ Abstractions for tool use (search, calculators, databases)

✦ Memory helpers (vector stores, summarised chat history)

✦ Event loops so the model can iterate towards a goal

Great, until you need something custom—e.g. streaming partial results to a dashboard or enforcing a stringent PII-removal policy. Then the abstraction leaks and you’re neck-deep in the framework’s internal callbacks. The rule of thumb I’ve learnt is:

Rapid prototype with a framework; scale-up with something you understand end-to-end.

Often that ends up being a minimalist agent loop you wrote yourself: take user input → build prompt → call model → parse output → decide next tool → repeat. Less wizardry, more control.

Practical downsides you will hit
Issue	What it looks like in practice	Mitigation
Hallucinations	Fake citations, invented product features	Post-check claims, plug in retrieval (RAG), keep prompts factual
Brittle tool calls	Wrong parameters, JSON that doesn’t parse	Enforce strict output schemas, validate before execution
Run-to-run variance	Passes the test Monday, fails Tuesday	Seed the RNG where possible; run multiple trials and take majority
Debugging pain	Logs full of prose, unclear causal chain	Add step-by-step logging; print the whole prompt & model reply
Latency & cost	Ten calls per loop × large model = $$	Cache intermediate results; use smaller models for “cheap” reasoning

The upside: why I still build with agents
Natural-language glue
Agents shine when you want to orchestrate many tools through one interface your non-tech colleagues can read: “Summarise these customer tickets, then open a Jira issue if you spot a recurring bug.” Wiring that logic in Python is a slog; in an agent prompt it’s two lines.

Rapid prototyping
I’ve mocked up data-cleaning workflows or SEO audits in an afternoon, letting the LLM handle edge cases I hadn’t even thought of. Perfect? No. Useful proof-of-concept? Absolutely.

Emergent reasoning
Chain-of-thought prompting plus tool use occasionally produces delightfully creative solutions—a reminder that pattern-matching at scale can mimic reasoning. I once asked an agent to identify an outlier in sales data and it not only flagged the row but suggested a plausible (and correct) business explanation pulled from analyst notes.

Continuous improvement
Because an agent’s “source code” is largely text,** iteration is cheap.** Tweaking one sentence in a system message can fix behaviour that would otherwise require refactoring a traditional program.

How to keep it real (and productive)
Start with a tight, testable goal.
“Email a weekly report from X to Y” is easier to debug than “optimise my business”.

Instrument everything.
Log the full prompt, model settings, intermediate tool outputs, and final answer. Future-you will thank present-you.

Evaluate like a scientist.
Run the same scenario ten times, measure accuracy, latency, cost. Change one variable at a time.

Mix and match models.
A smaller, cheaper model can triage simple steps, handing complex questions to the expensive model. Multi-agent, multi-model systems save money and often perform better.

Keep a human-in-the-loop for high-stakes tasks.
Legal letters, patient advice, or anything regulator-sensitive should pass a human review. Think of the agent as a tireless junior assistant, not an autonomous lawyer or doctor.

Final thoughts
LLM agents sit in an awkward middle ground: too powerful to ignore, too erratic to trust blindly. They’re autocomplete engines dressed up as junior staffers, bolstered by clever prompting and tool integration. If you embrace the experimental nature—monitor, measure, iterate—they can be game-changing productivity boosters. If you treat them as plug-and-play magic, you’ll spend more time cleaning up after them than enjoying the gains.

So stay curious, stay sceptical, and keep that evaluation notebook handy. With patience and a dash of engineering rigour, you can turn next-token prediction into next-level workflows—without falling for the hype.

Light references for further reading
Vaswani, A. et al. “Attention Is All You Need.” NeurIPS, 2017.

Kaplan, J. et al. “Scaling Laws for Neural Language Models.” OpenAI, 2020.

Wei, J. et al. “Chain-of-Thought Prompting Elicits Reasoning.” arXiv, 2022.

Yao, S. et al. “ReAct: Synergising Reasoning and Acting in Language Models.” 2023.

Chen, A. “Why Smart Developers Are Moving Off LangChain.” Medium, 2024.

(All links retrieved May 2025; accessible via arXiv or the authors’ blogs.)</span>"
  </div>
  
</div>
  </main>

  <script>
    var typed = new Typed("#typed", {
      stringsElement: "#strings",
      backSpeed: 10,
      typeSpeed: 30,
      backDelay: 1000,
      loop: true,
      smartBackspace: true,
    });
  </script>
</body>

</html>